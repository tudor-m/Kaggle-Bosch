print(c("min_child_weight: ",min_child_w))
nround = 150
param <- list(
#objective           = "multi:softprob", num_class = 4,
#objective           = "binary:logistic",
objective           = "reg:linear",
booster             = "gbtree",
#booster             = "gblinear",
base_score          = 0,
eta                 = 0.8,#0.05, #0.02, # 0.06, #0.01,
max_depth           = max_d, #changed from default of 8
subsample           = 0.9, #0.9, # 0.7
colsample_bytree    = 0.9, # 0.7
#num_parallel_tree   = 2,
nthread = 4,
alpha = 0,    #0.0001,
lambda = 0,
gamma = 0,
scale_pos_weight = 1,
min_child_weight    = min_child_w, #4, #4
#eval_metric         = mccEval,
#eval_metric         = splitEval5,
eval_metric         = splitEval,
#eval_metric         = "rmse",
early_stopping_rounds    = 2,
maximize = TRUE
)
set.seed(100)
fit.dev = xgb.train(params=param,dtrain,nrounds=nround,print.every.n = 2,maximize = FALSE,watchlist)
}
}
source('H:/user/projects/axomath/Kaggle-Bosch/R/wip.001.R')
##------ Sat Sep 10 19:40:12 2016 ------##
source('H:/user/projects/axomath/Kaggle-Bosch/R/wip.001-4.R')
##------ Sat Sep 10 19:54:28 2016 ------##
length(fit.dev.xgb.model)
list(ls())
rm(list(ls()))
remove(list(ls()))
remove(c(ls()))
list(ls())
remove(list = (ls()))
gc()
if (1 == 1)
{
rm(list=ls())
gc()
}
library(data.table)
library(xgboost)
library(stringi)
source("futil.R")
VERBOSE = 1
sink(file="output.R.txt",append = FALSE, split=TRUE)
timestamp()
##------ Fri Sep 23 23:07:35 2016 ------##
train.num.plant = getDataT("train","train.num.plant")
train.num.response = getDataT("train","train.num.response")
train.cat.plant = getDataT("train","train.cat.plant")
# Modelling
numrows = -1;
# Load all the input file:
train.num = fread('../data/train_numeric.csv',header = TRUE,nrows = numrows)
source('H:/user/projects/axomath/Kaggle-Bosch/R/buildPlantMap.R')
if (1 == 1)
{
rm(list=ls())
gc()
}
library(data.table)
library(xgboost)
library(stringi)
source("futil.R")
VERBOSE = 1
sink(file="output.R.txt",append = FALSE, split=TRUE)
timestamp()
##------ Sat Sep 24 06:05:35 2016 ------##
train.num.plant = getDataT("train","train.num.plant")
train.num.response = getDataT("train","train.num.response")
train.cat.plant = getDataT("train","train.cat.plant")
# Modelling
numrows = -1;
# Load all the input file:
train.num = fread('../data/train_numeric.csv',header = TRUE,nrows = numrows)
#train.cat = fread('../data/train_categorical.csv',header = TRUE,nrows = numrows)
#train.dat = fread('../data/train_date.csv',header = TRUE,nrows = numrows)
#test.num = fread('../data/test_numeric.csv',header = TRUE,nrows = numrows)
#test.cat = fread('../data/test_categorical.csv',header = TRUE,nrows = numrows)
#test.dat = fread('../data/test_date.csv',header = TRUE,nrows = numrows)
idxrows1 = 1:200000
idxrows2 = subSample(train.num$Response[idxrows1],5,1000)
idxrows3 = 200001:400000
#idxrows = which(train.num.plant$L1==1 & train.num.plant$L0==0 & train.num.plant$L2==0 & train.num.plant$L4==0)
#idxcols = 169:681 #L1 related
train.num1 = train.num[idxrows1,]
train.num2 = train.num[idxrows2,] # subsampled
train.num3 = train.num[idxrows3,] # cv
train.num1$Response = train.num.response[idxrows1]
train.num2$Response = train.num.response[idxrows2]
train.num3$Response = train.num.response[idxrows3]
remove(train.num);
gc()
set.seed(100)
fit.dev.xgb.model=list()
# remove(train.num);
#gc()
for (thr in seq(0.25,0.25,0.05))
for (i in 1:1)
{
# Model1 XGB:
# Data:
if (i==1)
{
#train.num = fread('../data/train_numeric.csv',header = TRUE,nrows = numrows)
dtrain <- xgb.DMatrix(data = as.matrix(train.num1[,-c("Id","Response"),with=F]), label=train.num1$Response, missing = NA)
dtest  <- xgb.DMatrix(data = as.matrix(train.num3[,-c("Id","Response"),with=F]), label=train.num3$Response, missing = NA)
#remove(train.num);
#gc()
}
# Fit:
watchlist <- list(train = dtrain, test = dtest)
mccEval <- function(preds, dtrain)
{
labels = getinfo(dtrain, "label")
err = as.numeric(errMeasure4(preds,labels,thr))
return(list(metric="error",value=err))
}
for (min_child_w in 13:13) {
for (max_d in 13:13) {
print(c("max_d: ",max_d))
print(c("min_child_weight: ",min_child_w))
print(thr)
nround = 90
param <- list(
#objective           = "multi:softprob", num_class = 4,
objective           = "binary:logistic",
#objective           = "reg:linear",
booster             = "gbtree",
#booster             = "gblinear",
base_score          = 0.5,
eta                 = 0.02,#0.05, #0.02, # 0.06, #0.01,
max_depth           = max_d, #changed from default of 8
subsample           = 0.9, #0.9, # 0.7
colsample_bytree    = 0.9, # 0.7
#num_parallel_tree   = 2,
nthread = 4,
alpha = 0,    #0.0001,
lambda = 0,
gamma = 0,
scale_pos_weight = 1,
min_child_weight    = min_child_w, #4, #4
eval_metric         = mccEval,
#eval_metric         = "rmse",
early_stopping_rounds    = 2,
maximize = TRUE
)
set.seed(100)
fit.dev = xgb.train(params=param,dtrain,nrounds=nround,print.every.n = 2,maximize = FALSE,watchlist)
}
}
# Model:
fit.dev.xgb.model[[i]] = fit.dev
}
remove(dtrain)
remove(dtest)
remove(fit.dev)
gc()
for (thr in seq(0.25,0.25,0.05))
for (i in 1:1)
{
# Model1 XGB:
# Data:
if (i==1)
{
#train.num = fread('../data/train_numeric.csv',header = TRUE,nrows = numrows)
dtrain <- xgb.DMatrix(data = as.matrix(train.num2[,-c("Id","Response"),with=F]), label=train.num2$Response, missing = NA)
dtest  <- xgb.DMatrix(data = as.matrix(train.num3[,-c("Id","Response"),with=F]), label=train.num3$Response, missing = NA)
#remove(train.num);
#gc()
}
# Fit:
watchlist <- list(train = dtrain, test = dtest)
mccEval <- function(preds, dtrain)
{
labels = getinfo(dtrain, "label")
err = as.numeric(errMeasure4(preds,labels,thr))
return(list(metric="error",value=err))
}
for (min_child_w in 13:13) {
for (max_d in 13:13) {
print(c("max_d: ",max_d))
print(c("min_child_weight: ",min_child_w))
print(thr)
nround = 90
param <- list(
#objective           = "multi:softprob", num_class = 4,
objective           = "binary:logistic",
#objective           = "reg:linear",
booster             = "gbtree",
#booster             = "gblinear",
base_score          = 0.5,
eta                 = 0.02,#0.05, #0.02, # 0.06, #0.01,
max_depth           = max_d, #changed from default of 8
subsample           = 0.9, #0.9, # 0.7
colsample_bytree    = 0.9, # 0.7
#num_parallel_tree   = 2,
nthread = 4,
alpha = 0,    #0.0001,
lambda = 0,
gamma = 0,
scale_pos_weight = 1,
min_child_weight    = min_child_w, #4, #4
eval_metric         = mccEval,
#eval_metric         = "rmse",
early_stopping_rounds    = 2,
maximize = TRUE
)
set.seed(100)
fit.dev = xgb.train(params=param,dtrain,nrounds=nround,print.every.n = 2,maximize = FALSE,watchlist)
}
}
# Model:
fit.dev.xgb.model[[i]] = fit.dev
}
for (thr in seq(0.25,0.75,0.05))
for (i in 1:1)
{
# Model1 XGB:
# Data:
if (i==1)
{
#train.num = fread('../data/train_numeric.csv',header = TRUE,nrows = numrows)
dtrain <- xgb.DMatrix(data = as.matrix(train.num2[,-c("Id","Response"),with=F]), label=train.num2$Response, missing = NA)
dtest  <- xgb.DMatrix(data = as.matrix(train.num3[,-c("Id","Response"),with=F]), label=train.num3$Response, missing = NA)
#remove(train.num);
#gc()
}
# Fit:
watchlist <- list(train = dtrain, test = dtest)
mccEval <- function(preds, dtrain)
{
labels = getinfo(dtrain, "label")
err = as.numeric(errMeasure4(preds,labels,thr))
return(list(metric="error",value=err))
}
for (min_child_w in 13:13) {
for (max_d in 13:13) {
print(c("max_d: ",max_d))
print(c("min_child_weight: ",min_child_w))
print(thr)
nround = 90
param <- list(
#objective           = "multi:softprob", num_class = 4,
objective           = "binary:logistic",
#objective           = "reg:linear",
booster             = "gbtree",
#booster             = "gblinear",
base_score          = 0.5,
eta                 = 0.02,#0.05, #0.02, # 0.06, #0.01,
max_depth           = max_d, #changed from default of 8
subsample           = 0.9, #0.9, # 0.7
colsample_bytree    = 0.9, # 0.7
#num_parallel_tree   = 2,
nthread = 4,
alpha = 0,    #0.0001,
lambda = 0,
gamma = 0,
scale_pos_weight = 1,
min_child_weight    = min_child_w, #4, #4
eval_metric         = mccEval,
#eval_metric         = "rmse",
early_stopping_rounds    = 2,
maximize = TRUE
)
set.seed(100)
fit.dev = xgb.train(params=param,dtrain,nrounds=nround,print.every.n = 2,maximize = FALSE,watchlist)
}
}
# Model:
fit.dev.xgb.model[[i]] = fit.dev
}
for (min_child_w in 13:13) {
for (max_d in 13:13) {
print(c("max_d: ",max_d))
print(c("min_child_weight: ",min_child_w))
print(thr)
nround = 150
param <- list(
#objective           = "multi:softprob", num_class = 4,
objective           = "binary:logistic",
#objective           = "reg:linear",
booster             = "gbtree",
#booster             = "gblinear",
base_score          = 0.5,
eta                 = 0.05,#0.05, #0.02, # 0.06, #0.01,
max_depth           = max_d, #changed from default of 8
subsample           = 0.9, #0.9, # 0.7
colsample_bytree    = 0.9, # 0.7
#num_parallel_tree   = 2,
nthread = 4,
alpha = 0,    #0.0001,
lambda = 0,
gamma = 0,
scale_pos_weight = 1,
min_child_weight    = min_child_w, #4, #4
eval_metric         = mccEval,
#eval_metric         = "rmse",
early_stopping_rounds    = 2,
maximize = TRUE
)
set.seed(100)
fit.dev = xgb.train(params=param,dtrain,nrounds=nround,print.every.n = 2,maximize = FALSE,watchlist)
}
}
for (min_child_w in 10:15) {
for (max_d in 10:15) {
print(c("max_d: ",max_d))
print(c("min_child_weight: ",min_child_w))
print(thr)
nround = 150
param <- list(
#objective           = "multi:softprob", num_class = 4,
objective           = "binary:logistic",
#objective           = "reg:linear",
booster             = "gbtree",
#booster             = "gblinear",
base_score          = 0.5,
eta                 = 0.05,#0.05, #0.02, # 0.06, #0.01,
max_depth           = max_d, #changed from default of 8
subsample           = 0.9, #0.9, # 0.7
colsample_bytree    = 0.9, # 0.7
#num_parallel_tree   = 2,
nthread = 4,
alpha = 0,    #0.0001,
lambda = 0,
gamma = 0,
scale_pos_weight = 1,
min_child_weight    = min_child_w, #4, #4
eval_metric         = mccEval,
#eval_metric         = "rmse",
early_stopping_rounds    = 2,
maximize = TRUE
)
set.seed(100)
fit.dev = xgb.train(params=param,dtrain,nrounds=nround,print.every.n = 2,maximize = FALSE,watchlist)
}
}
sink()
sink()
sink.number()
sink()
sink.number()
sink(file="output.R.txt",append = FALSE, split=TRUE)
timestamp()
##------ Sat Sep 24 06:53:23 2016 ------##
for (thr in seq(0.25,0.75,0.05))
for (i in 1:1)
{
# Model1 XGB:
# Data:
if (i==1)
{
#train.num = fread('../data/train_numeric.csv',header = TRUE,nrows = numrows)
dtrain <- xgb.DMatrix(data = as.matrix(train.num2[,-c("Id","Response"),with=F]), label=train.num2$Response, missing = NA)
dtest  <- xgb.DMatrix(data = as.matrix(train.num3[,-c("Id","Response"),with=F]), label=train.num3$Response, missing = NA)
#remove(train.num);
#gc()
}
# Fit:
watchlist <- list(train = dtrain, test = dtest)
mccEval <- function(preds, dtrain)
{
labels = getinfo(dtrain, "label")
err = as.numeric(errMeasure4(preds,labels,thr))
return(list(metric="error",value=err))
}
for (min_child_w in 1:17) {
for (max_d in 1:17) {
print(c("max_d: ",max_d))
print(c("min_child_weight: ",min_child_w))
print(thr)
nround = 80
param <- list(
#objective           = "multi:softprob", num_class = 4,
objective           = "binary:logistic",
#objective           = "reg:linear",
booster             = "gbtree",
#booster             = "gblinear",
base_score          = 0.5,
eta                 = 0.05,#0.05, #0.02, # 0.06, #0.01,
max_depth           = max_d, #changed from default of 8
subsample           = 0.9, #0.9, # 0.7
colsample_bytree    = 0.9, # 0.7
#num_parallel_tree   = 2,
nthread = 4,
alpha = 0,    #0.0001,
lambda = 0,
gamma = 0,
scale_pos_weight = 1,
min_child_weight    = min_child_w, #4, #4
eval_metric         = mccEval,
#eval_metric         = "rmse",
early_stopping_rounds    = 2,
maximize = TRUE
)
set.seed(100)
fit.dev = xgb.train(params=param,dtrain,nrounds=nround,print.every.n = 2,maximize = FALSE,watchlist)
}
}
# Model:
fit.dev.xgb.model[[i]] = fit.dev
}
seq(1,21,4)
sink()
sink()
sink(file="output.R.txt",append = FALSE, split=TRUE)
timestamp()
##------ Sat Sep 24 07:43:49 2016 ------##
for (thr in seq(0.25,0.75,0.1))
for (i in 1:1)
{
# Model1 XGB:
# Data:
if (i==1)
{
#train.num = fread('../data/train_numeric.csv',header = TRUE,nrows = numrows)
dtrain <- xgb.DMatrix(data = as.matrix(train.num2[,-c("Id","Response"),with=F]), label=train.num2$Response, missing = NA)
dtest  <- xgb.DMatrix(data = as.matrix(train.num3[,-c("Id","Response"),with=F]), label=train.num3$Response, missing = NA)
#remove(train.num);
#gc()
}
# Fit:
watchlist <- list(train = dtrain, test = dtest)
mccEval <- function(preds, dtrain)
{
labels = getinfo(dtrain, "label")
err = as.numeric(errMeasure4(preds,labels,thr))
return(list(metric="error",value=err))
}
for (min_child_w in seq(1,21,4)) {
for (max_d in seq(1,21,4)) {
print(c("max_d: ",max_d))
print(c("min_child_weight: ",min_child_w))
print(thr)
nround = 80
param <- list(
#objective           = "multi:softprob", num_class = 4,
objective           = "binary:logistic",
#objective           = "reg:linear",
booster             = "gbtree",
#booster             = "gblinear",
base_score          = 0.5,
eta                 = 0.05,#0.05, #0.02, # 0.06, #0.01,
max_depth           = max_d, #changed from default of 8
subsample           = 0.9, #0.9, # 0.7
colsample_bytree    = 0.9, # 0.7
#num_parallel_tree   = 2,
nthread = 4,
alpha = 0,    #0.0001,
lambda = 0,
gamma = 0,
scale_pos_weight = 1,
min_child_weight    = min_child_w, #4, #4
eval_metric         = mccEval,
#eval_metric         = "rmse",
early_stopping_rounds    = 2,
maximize = TRUE
)
set.seed(100)
fit.dev = xgb.train(params=param,dtrain,nrounds=nround,print.every.n = 2,maximize = FALSE,watchlist)
}
}
# Model:
fit.dev.xgb.model[[i]] = fit.dev
}
thr
thr=0.55
seq(21,29,4)
for (min_child_w in seq(21,29,4)) {
for (max_d in seq(21,29,4)) {
print(c("max_d: ",max_d))
print(c("min_child_weight: ",min_child_w))
print(thr)
nround = 80
param <- list(
#objective           = "multi:softprob", num_class = 4,
objective           = "binary:logistic",
#objective           = "reg:linear",
booster             = "gbtree",
#booster             = "gblinear",
base_score          = 0.5,
eta                 = 0.05,#0.05, #0.02, # 0.06, #0.01,
max_depth           = max_d, #changed from default of 8
subsample           = 0.9, #0.9, # 0.7
colsample_bytree    = 0.9, # 0.7
#num_parallel_tree   = 2,
nthread = 4,
alpha = 0,    #0.0001,
lambda = 0,
gamma = 0,
scale_pos_weight = 1,
min_child_weight    = min_child_w, #4, #4
eval_metric         = mccEval,
#eval_metric         = "rmse",
early_stopping_rounds    = 2,
maximize = TRUE
)
set.seed(100)
fit.dev = xgb.train(params=param,dtrain,nrounds=nround,print.every.n = 2,maximize = FALSE,watchlist)
}
}
sink()
